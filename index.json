[{"content":"Theory In large language model reinforcement learning training, there are two ways to incorporate KL divergence constraints: reward shaping and KL loss. The former directly adds KL estimation to the reward value as a new reward $r \\leftarrow r - \\beta\\cdot KL$; the latter puts the KL estimation term in the loss function, computing $\\nabla_\\theta \\operatorname{KL}(\\pi_\\theta||\\pi_{ref})$ for backpropagation.\nSchulman (2020) proposed three methods for estimating KL divergence, among which k3 loss is considered a good estimator with both unbiased and low variance properties. DeepSeek\u0026rsquo;s GRPO algorithm (Guo et al. 2025) adopted this approach:\nFig. 1. GRPO Algorithm. (Source: Guo et al. 2025)\nHowever, when using KL Loss, we actually estimate the gradient of KL divergence through sampling, which differs somewhat from Schulman (2020)\u0026rsquo;s analysis. In training, after constructing the estimator, we directly use it as a loss for backpropagation, hoping it remains a good approximation:\n$$ \\begin{align*} \\nabla_\\theta \\widehat{\\operatorname{KL}}(X) \u0026amp;\\approx \\nabla_\\theta \\operatorname{KL}(\\pi_\\theta||\\pi_{\\theta_{ref}})=\\nabla_\\theta \\int \\pi_\\theta(x)\\cdot\\log\\left(\\frac{\\pi_\\theta(x)}{\\pi_{ref}(x)}\\right)dx\\\\ \u0026amp;= \\int \\nabla_\\theta\\pi_\\theta(x)\\cdot\\log\\left(\\frac{\\pi_\\theta(x)}{\\pi_{ref}(x)}\\right) + \\pi_\\theta(x)\\nabla_\\theta\\log\\pi_\\theta(x)dx\\\\ \u0026amp;=\\int \\pi_\\theta(x)\\cdot\\nabla_\\theta\\log\\pi_\\theta(x)\\log\\left(\\frac{\\pi_\\theta(x)}{\\pi_{ref}(x)}\\right) + \\pi_\\theta(x)\\cdot \\frac{1}{\\pi_\\theta(x)}\\nabla_\\theta\\pi_\\theta(x)dx\\\\ \u0026amp;=E_{x\\sim\\pi_\\theta}\\left[\\nabla_\\theta\\log\\pi_\\theta(X)\\cdot\\log\\left(\\frac{\\pi_\\theta(X)}{\\pi_{ref}(X)}\\right)\\right]. \\end{align*} $$\nHowever, the gradient of an unbiased KL estimator is not necessarily an unbiased estimator of the KL gradient:\n$$ E_{\\pi_\\theta}[\\nabla_\\theta \\widehat{\\operatorname{KL}}(X)] \\neq \\nabla_\\theta E_{\\pi_\\theta}[ \\widehat{\\operatorname{KL}}(X)] = \\nabla_\\theta \\operatorname{KL}(\\pi_\\theta||\\pi_{\\theta_{ref}}). $$\nIn fact, they differ by one term:\n$$ \\begin{align*} \\nabla_\\theta E_{\\pi_\\theta}[f_\\theta(X)] \u0026amp;= \\int \\nabla_\\theta\\pi_\\theta(x)\\cdot f_\\theta(x) + \\pi_\\theta(x)\\cdot\\nabla_\\theta f_\\theta(x) dx\\\\ \u0026amp;= \\int \\pi_\\theta(x)\\cdot\\nabla_\\theta \\log\\pi_\\theta(x)\\cdot f_\\theta(x) dx + \\int \\pi_\\theta(x)\\cdot\\nabla_\\theta f_\\theta(x) dx\\\\ \u0026amp;= E_{\\pi_\\theta}[\\nabla_\\theta\\log\\pi_\\theta(X)\\cdot f_\\theta(X)] + E_{\\pi_\\theta}[\\nabla_\\theta f_\\theta(X)]. \\end{align*} $$\nFor samples $X\\sim\\pi_\\theta(\\cdot)$ generated by the policy, taking $\\widehat{\\operatorname{KL}}$ as k1, k2, and k3 loss respectively:\nk1\n$$ \\begin{align*} \\widehat{\\operatorname{KL}}(X) \u0026amp;= \\log\\pi_\\theta(X) - \\log\\pi_{ref}(X);\\\\ \\nabla_\\theta \\widehat{\\operatorname{KL}}(X) \u0026amp;= \\nabla_\\theta\\log\\pi_\\theta(X)；\\\\ E_{\\pi_\\theta}[\\nabla_\\theta \\widehat{\\operatorname{KL}}(X)] \u0026amp;= \\int \\pi_\\theta(x)\\cdot \\frac{1}{\\pi_\\theta(x)}\\nabla_\\theta\\pi_\\theta(x)dx=0 \\neq \\nabla_\\theta \\operatorname{KL}(\\pi_\\theta||\\pi_{\\theta_{ref}}). \\end{align*} $$\nk2\n$$ \\begin{align*} \\widehat{\\operatorname{KL}}(X) \u0026amp;= \\frac{1}{2}(\\log\\pi_\\theta(X) - \\log\\pi_{ref}(X))^2;\\\\ \\nabla_\\theta \\widehat{\\operatorname{KL}}(X) \u0026amp;= \\nabla_\\theta\\log\\pi_\\theta(X)\\cdot\\log\\left(\\frac{\\pi_\\theta(X)}{\\pi_{ref}(X)}\\right)；\\\\ E_{\\pi_\\theta}[\\nabla_\\theta \\widehat{\\operatorname{KL}}(X)] \u0026amp;= E_{\\pi_\\theta}\\left[\\nabla_\\theta\\log\\pi_\\theta(X) \\cdot \\log\\left(\\frac{\\pi_\\theta(X)}{\\pi_{ref}(X)}\\right)\\right] = \\nabla_\\theta \\operatorname{KL}(\\pi_\\theta||\\pi_{\\theta_{ref}}). \\end{align*} $$\nk3\n$$ \\begin{align*} \\widehat{\\operatorname{KL}}(X) \u0026amp;= \\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)} - 1 - \\log\\left(\\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)}\\right);\\\\ \\nabla_\\theta \\widehat{\\operatorname{KL}}(X) \u0026amp;= \\nabla_\\theta\\log\\pi_\\theta(X) \\cdot \\left(1 - \\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)}\\right);\\\\ E_{\\pi_\\theta}[\\nabla_\\theta \\widehat{\\operatorname{KL}}(X)] \u0026amp;= E_{\\pi_\\theta}\\left[\\nabla_\\theta\\log\\pi_\\theta(X) \\cdot \\left(1 - \\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)}\\right)\\right]\\neq \\nabla_\\theta \\operatorname{KL}(\\pi_\\theta||\\pi_{\\theta_{ref}}). \\end{align*} $$\nAs we can see, although k2 loss is not an unbiased estimator of KL divergence, its gradient is an unbiased estimator of KL gradient; while k1 and k3 are the opposite. Notably, the expectation of k1 loss gradient is always zero, which becomes trivial under large batch sizes. This also indicates that the KL loss adopted in the GRPO algorithm is not a good estimator.\nSometimes people also use reverse KL divergence, which swaps the positions of policy model and reference model in the KL divergence calculation. Since samples $X$ are always drawn from the policy model, we need to multiply by a probability ratio:\n$$ \\begin{align*} \\operatorname{KL}(\\pi_{ref}||\\pi_\\theta) \u0026amp;= E_{X\\sim\\pi_{ref}}\\left[ \\log\\left(\\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)}\\right) \\right] = E_{X\\sim\\pi_{\\theta}}\\left[ \\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)} \\cdot \\log\\left(\\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)}\\right) \\right]; \\\\ \\nabla_\\theta \\operatorname{KL}(\\pi_{ref}||\\pi_\\theta) \u0026amp;= E_{X\\sim\\pi_{ref}} [-\\nabla_\\theta \\log \\pi_\\theta(X)] = E_{X\\sim\\pi_{\\theta}}\\left[ - \\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)} \\cdot \\nabla_\\theta \\log \\pi_\\theta(X) \\right] \\\\ \u0026amp;= E_{X\\sim\\pi_{\\theta}}\\left[ \\left(- \\frac{\\pi_{ref}(X) }{\\pi_\\theta(X)^2} \\right) \\cdot \\nabla_\\theta \\pi_\\theta(X) \\right] \\\\ \u0026amp;= E_{X\\sim\\pi_{\\theta}}\\left[ \\nabla_\\theta \\left(\\frac{\\pi_{ref}(X) }{\\pi_\\theta(X)} \\right) \\right]. \\end{align*} $$\nTherefore, if we want to estimate its gradient in KL loss, an unbiased estimator can be:\n$$ \\widehat{\\operatorname{KL}}(X) = \\frac{\\pi_{ref}(X) }{\\pi_\\theta(X)}. $$\nIn this case, the gradient of the unbiased KL estimator is still not equal to the unbiased estimator of KL gradient:\n$$ \\begin{align*} \\widehat{\\operatorname{KL}}(X) \u0026amp;= \\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)} \\cdot \\log\\left(\\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)}\\right); \\\\ E_{X\\sim\\pi_{\\theta}}[\\nabla_\\theta \\widehat{\\operatorname{KL}}(X)] \u0026amp;= E_{X\\sim\\pi_{\\theta}} \\left[ \\left(\\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)}\\right) \\cdot (- \\nabla_\\theta\\log\\pi_\\theta(X)) \\cdot \\left( 1 + \\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)}\\right) \\right] \\\\ \u0026amp;= E_{X\\sim\\pi_{ref}} \\left[ (- \\nabla_\\theta\\log\\pi_\\theta(X)) \\cdot \\left( 1 + \\frac{\\pi_{ref}(X)}{\\pi_\\theta(X)}\\right) \\right] \\\\ \u0026amp;\\neq E_{X\\sim\\pi_{ref}} [-\\nabla_\\theta \\log \\pi_\\theta(X)] = \\nabla_\\theta \\operatorname{KL}(\\pi_{ref}||\\pi_\\theta). \\end{align*} $$\nExperiments Consider $\\pi_\\theta=\\mathcal{N}(0,1),~\\pi_{ref}=\\mathcal{N}(1,1.1)$, where parameter $\\theta$ represents the mean and standard deviation of the Gaussian distribution. The experimental results align with theoretical analysis:\nFig. 2. Gradient Estimation of KL\nAverage gradient value comparison (mean parameter): True KL gradient: -0.826446 k1 average gradient: 0.000137 k2 average gradient: -0.826291 k3 average gradient: -0.999618 Average gradient value comparison (standard deviation parameter): True KL gradient: -0.173554 k1 average gradient: -0.000104 k2 average gradient: -0.173379 k3 average gradient: -1.209091 Reverse KL - Average gradient value comparison (mean parameter): True Reverse KL gradient: -1.000000 reverse_k1 average gradient: -2.718389 reverse_k2 average gradient: -0.999755 Reverse KL - Average gradient value comparison (standard deviation parameter): True Reverse KL gradient: -1.210000 reverse_k1 average gradient: -4.496196 reverse_k2 average gradient: -1.208987 KL - Comparison of bias and standard deviation for mean parameter gradients: ╒════╤═════════════╤══════════════╕ │ │ bias/true │ stdev/true │ ╞════╪═════════════╪══════════════╡ │ k1 │ 1.0002 │ 0.0054 │ ├────┼─────────────┼──────────────┤ │ k2 │ 0.0002 │ 0.0065 │ ├────┼─────────────┼──────────────┤ │ k3 │ 0.2095 │ 0.026 │ ╘════╧═════════════╧══════════════╛ KL - Comparison of bias and standard deviation for standard deviation parameter gradients: ╒════╤═════════════╤══════════════╕ │ │ bias/true │ stdev/true │ ╞════╪═════════════╪══════════════╡ │ k1 │ 0.9994 │ 0.0359 │ ├────┼─────────────┼──────────────┤ │ k2 │ 0.001 │ 0.0686 │ ├────┼─────────────┼──────────────┤ │ k3 │ 5.9667 │ 0.4311 │ ╘════╧═════════════╧══════════════╛ Reverse KL - Comparison of bias and standard deviation for mean parameter gradients: ╒════════════╤═════════════╤══════════════╕ │ │ bias/true │ stdev/true │ ╞════════════╪═════════════╪══════════════╡ │ reverse_k1 │ 1.7184 │ 0.1081 │ ├────────────┼─────────────┼──────────────┤ │ reverse_k2 │ 0.0002 │ 0.0231 │ ╘════════════╧═════════════╧══════════════╛ Reverse KL - Comparison of bias and standard deviation for standard deviation parameter gradients: ╒════════════╤═════════════╤══════════════╕ │ │ bias/true │ stdev/true │ ╞════════════╪═════════════╪══════════════╡ │ reverse_k1 │ 2.7159 │ 0.3419 │ ├────────────┼─────────────┼──────────────┤ │ reverse_k2 │ 0.0008 │ 0.0636 │ ╘════════════╧═════════════╧══════════════╛ Code import torch import numpy as np import matplotlib.pyplot as plt from torch.distributions import Normal import seaborn as sns # Set random seed for reproducibility torch.manual_seed(42) np.random.seed(42) # Define parameters for two normal distributions def setup_distributions(mu_theta=0.0, sigma_theta=1.0, mu_ref=1.0, sigma_ref=1.1): # Create trainable parameters mu_theta_param = torch.tensor(mu_theta, requires_grad=True) sigma_theta_param = torch.tensor(sigma_theta, requires_grad=True) # Reference distribution parameters (fixed) mu_ref_param = torch.tensor(mu_ref) sigma_ref_param = torch.tensor(sigma_ref) # Create distributions pi_theta = Normal(mu_theta_param, sigma_theta_param) pi_ref = Normal(mu_ref_param, sigma_ref_param) return pi_theta, pi_ref, mu_theta_param, sigma_theta_param # Calculate the true KL divergence (analytical solution for normal distributions) def true_kl_divergence(pi_theta, pi_ref): mu_theta = pi_theta.loc sigma_theta = pi_theta.scale mu_ref = pi_ref.loc sigma_ref = pi_ref.scale kl = (torch.log(sigma_ref/sigma_theta) + (sigma_theta**2 + (mu_theta - mu_ref)**2)/(2*sigma_ref**2) - 0.5) rkl = (torch.log(sigma_theta/sigma_ref) + (sigma_ref**2 + (mu_ref - mu_theta)**2)/(2*sigma_theta**2) - 0.5) return kl, rkl # Three different KL divergence estimates def k1_loss(x, pi_theta, pi_ref): return pi_theta.log_prob(x) - pi_ref.log_prob(x) def k2_loss(x, pi_theta, pi_ref): return 0.5 * (pi_theta.log_prob(x) - pi_ref.log_prob(x))**2 def k3_loss(x, pi_theta, pi_ref): ratio = torch.exp(pi_ref.log_prob(x) - pi_theta.log_prob(x)) return ratio - 1 - torch.log(ratio) # Two different reverse KL divergence estimates def reverse_k1_loss(x, pi_theta, pi_ref): ratio = torch.exp(pi_ref.log_prob(x) - pi_theta.log_prob(x)) return ratio * (pi_ref.log_prob(x) - pi_theta.log_prob(x)) def reverse_k2_loss(x, pi_theta, pi_ref): return torch.exp(pi_ref.log_prob(x) - pi_theta.log_prob(x)) # Sample and compute gradients def estimate_gradients(sample_size=10000, num_trials=10): pi_theta, pi_ref, mu_param, sigma_param = setup_distributions() # Compute the gradient of the true and reverse KL divergence true_kl, true_reverse_kl = true_kl_divergence(pi_theta, pi_ref) true_kl.backward() true_grad_mu = mu_param.grad.item() true_grad_sigma = sigma_param.grad.item() mu_param.grad.zero_() sigma_param.grad.zero_() true_reverse_kl.backward() true_reverse_grad_mu = mu_param.grad.item() true_reverse_grad_sigma = sigma_param.grad.item() mu_param.grad.zero_() sigma_param.grad.zero_() # Store gradients from different estimates k1_grads_mu = [] k1_grads_sigma = [] k2_grads_mu = [] k2_grads_sigma = [] k3_grads_mu = [] k3_grads_sigma = [] reverse_k1_grads_mu = [] reverse_k1_grads_sigma = [] reverse_k2_grads_mu = [] reverse_k2_grads_sigma = [] for _ in range(num_trials): pi_theta, pi_ref, mu_param, sigma_param = setup_distributions() # Sample from the current policy samples = pi_theta.sample((sample_size,)) # Get gradient of KL estimation k1_values = k1_loss(samples, pi_theta, pi_ref) k1_mean = k1_values.mean() k1_mean.backward() k1_grads_mu.append(mu_param.grad.item()) k1_grads_sigma.append(sigma_param.grad.item()) mu_param.grad.zero_() sigma_param.grad.zero_() k2_values = k2_loss(samples, pi_theta, pi_ref) k2_mean = k2_values.mean() k2_mean.backward() k2_grads_mu.append(mu_param.grad.item()) k2_grads_sigma.append(sigma_param.grad.item()) mu_param.grad.zero_() sigma_param.grad.zero_() k3_values = k3_loss(samples, pi_theta, pi_ref) k3_mean = k3_values.mean() k3_mean.backward() k3_grads_mu.append(mu_param.grad.item()) k3_grads_sigma.append(sigma_param.grad.item()) mu_param.grad.zero_() sigma_param.grad.zero_() reverse_k1_values = reverse_k1_loss(samples, pi_theta, pi_ref) reverse_k1_mean = reverse_k1_values.mean() reverse_k1_mean.backward() reverse_k1_grads_mu.append(mu_param.grad.item()) reverse_k1_grads_sigma.append(sigma_param.grad.item()) mu_param.grad.zero_() sigma_param.grad.zero_() reverse_k2_values = reverse_k2_loss(samples, pi_theta, pi_ref) reverse_k2_mean = reverse_k2_values.mean() reverse_k2_mean.backward() reverse_k2_grads_mu.append(mu_param.grad.item()) reverse_k2_grads_sigma.append(sigma_param.grad.item()) mu_param.grad.zero_() sigma_param.grad.zero_() return { \u0026#39;true_grad_mu\u0026#39;: true_grad_mu, \u0026#39;true_grad_sigma\u0026#39;: true_grad_sigma, \u0026#39;k1_grads_mu\u0026#39;: k1_grads_mu, \u0026#39;k1_grads_sigma\u0026#39;: k1_grads_sigma, \u0026#39;k2_grads_mu\u0026#39;: k2_grads_mu, \u0026#39;k2_grads_sigma\u0026#39;: k2_grads_sigma, \u0026#39;k3_grads_mu\u0026#39;: k3_grads_mu, \u0026#39;k3_grads_sigma\u0026#39;: k3_grads_sigma, \u0026#39;true_reverse_grad_mu\u0026#39;: true_reverse_grad_mu, \u0026#39;true_reverse_grad_sigma\u0026#39;: true_reverse_grad_sigma, \u0026#39;reverse_k1_grads_mu\u0026#39;: reverse_k1_grads_mu, \u0026#39;reverse_k1_grads_sigma\u0026#39;: reverse_k1_grads_sigma, \u0026#39;reverse_k2_grads_mu\u0026#39;: reverse_k2_grads_mu, \u0026#39;reverse_k2_grads_sigma\u0026#39;: reverse_k2_grads_sigma } def create_nice_table(results): true_grad_mu = results[\u0026#39;true_grad_mu\u0026#39;] true_grad_sigma = results[\u0026#39;true_grad_sigma\u0026#39;] true_reverse_grad_mu = results[\u0026#39;true_reverse_grad_mu\u0026#39;] true_reverse_grad_sigma = results[\u0026#39;true_reverse_grad_sigma\u0026#39;] k1_bias_mu = abs(np.mean(results[\u0026#39;k1_grads_mu\u0026#39;]) - true_grad_mu) / abs(true_grad_mu) k2_bias_mu = abs(np.mean(results[\u0026#39;k2_grads_mu\u0026#39;]) - true_grad_mu) / abs(true_grad_mu) k3_bias_mu = abs(np.mean(results[\u0026#39;k3_grads_mu\u0026#39;]) - true_grad_mu) / abs(true_grad_mu) k1_std_mu = np.std(results[\u0026#39;k1_grads_mu\u0026#39;]) / abs(true_grad_mu) k2_std_mu = np.std(results[\u0026#39;k2_grads_mu\u0026#39;]) / abs(true_grad_mu) k3_std_mu = np.std(results[\u0026#39;k3_grads_mu\u0026#39;]) / abs(true_grad_mu) k1_bias_sigma = abs(np.mean(results[\u0026#39;k1_grads_sigma\u0026#39;]) - true_grad_sigma) / abs(true_grad_sigma) k2_bias_sigma = abs(np.mean(results[\u0026#39;k2_grads_sigma\u0026#39;]) - true_grad_sigma) / abs(true_grad_sigma) k3_bias_sigma = abs(np.mean(results[\u0026#39;k3_grads_sigma\u0026#39;]) - true_grad_sigma) / abs(true_grad_sigma) k1_std_sigma = np.std(results[\u0026#39;k1_grads_sigma\u0026#39;]) / abs(true_grad_sigma) k2_std_sigma = np.std(results[\u0026#39;k2_grads_sigma\u0026#39;]) / abs(true_grad_sigma) k3_std_sigma = np.std(results[\u0026#39;k3_grads_sigma\u0026#39;]) / abs(true_grad_sigma) reverse_k1_bias_mu = abs(np.mean(results[\u0026#39;reverse_k1_grads_mu\u0026#39;]) - true_reverse_grad_mu) / abs(true_reverse_grad_mu) reverse_k2_bias_mu = abs(np.mean(results[\u0026#39;reverse_k2_grads_mu\u0026#39;]) - true_reverse_grad_mu) / abs(true_reverse_grad_mu) reverse_k1_std_mu = np.std(results[\u0026#39;reverse_k1_grads_mu\u0026#39;]) / abs(true_reverse_grad_mu) reverse_k2_std_mu = np.std(results[\u0026#39;reverse_k2_grads_mu\u0026#39;]) / abs(true_reverse_grad_mu) reverse_k1_bias_sigma = abs(np.mean(results[\u0026#39;reverse_k1_grads_sigma\u0026#39;]) - true_reverse_grad_sigma) / abs(true_reverse_grad_sigma) reverse_k2_bias_sigma = abs(np.mean(results[\u0026#39;reverse_k2_grads_sigma\u0026#39;]) - true_reverse_grad_sigma) / abs(true_reverse_grad_sigma) reverse_k1_std_sigma = np.std(results[\u0026#39;reverse_k1_grads_sigma\u0026#39;]) / abs(true_reverse_grad_sigma) reverse_k2_std_sigma = np.std(results[\u0026#39;reverse_k2_grads_sigma\u0026#39;]) / abs(true_reverse_grad_sigma) # Create table from tabulate import tabulate import pandas as pd df_mu = pd.DataFrame({ \u0026#39;bias/true\u0026#39;: [k1_bias_mu, k2_bias_mu, k3_bias_mu], \u0026#39;stdev/true\u0026#39;: [k1_std_mu, k2_std_mu, k3_std_mu] }, index=[\u0026#39;k1\u0026#39;, \u0026#39;k2\u0026#39;, \u0026#39;k3\u0026#39;]) df_reverse_mu = pd.DataFrame({ \u0026#39;bias/true\u0026#39;: [reverse_k1_bias_mu, reverse_k2_bias_mu], \u0026#39;stdev/true\u0026#39;: [reverse_k1_std_mu, reverse_k2_std_mu] }, index=[\u0026#39;reverse_k1\u0026#39;, \u0026#39;reverse_k2\u0026#39;]) df_sigma = pd.DataFrame({ \u0026#39;bias/true\u0026#39;: [k1_bias_sigma, k2_bias_sigma, k3_bias_sigma], \u0026#39;stdev/true\u0026#39;: [k1_std_sigma, k2_std_sigma, k3_std_sigma] }, index=[\u0026#39;k1\u0026#39;, \u0026#39;k2\u0026#39;, \u0026#39;k3\u0026#39;]) df_reverse_sigma = pd.DataFrame({ \u0026#39;bias/true\u0026#39;: [reverse_k1_bias_sigma, reverse_k2_bias_sigma], \u0026#39;stdev/true\u0026#39;: [reverse_k1_std_sigma, reverse_k2_std_sigma] }, index=[\u0026#39;reverse_k1\u0026#39;, \u0026#39;reverse_k2\u0026#39;]) # Format values df_mu = df_mu.round(4) df_sigma = df_sigma.round(4) df_reverse_mu = df_reverse_mu.round(4) df_reverse_sigma = df_reverse_sigma.round(4) # Print nice table print(\u0026#34;KL - Comparison of bias and standard deviation for mean parameter gradients:\u0026#34;) print(tabulate(df_mu, headers=\u0026#39;keys\u0026#39;, tablefmt=\u0026#39;fancy_grid\u0026#39;)) print(\u0026#34;\\nKL - Comparison of bias and standard deviation for standard deviation parameter gradients:\u0026#34;) print(tabulate(df_sigma, headers=\u0026#39;keys\u0026#39;, tablefmt=\u0026#39;fancy_grid\u0026#39;)) print(\u0026#34;\\nReverse KL - Comparison of bias and standard deviation for mean parameter gradients:\u0026#34;) print(tabulate(df_reverse_mu, headers=\u0026#39;keys\u0026#39;, tablefmt=\u0026#39;fancy_grid\u0026#39;)) print(\u0026#34;\\nReverse KL - Comparison of bias and standard deviation for standard deviation parameter gradients:\u0026#34;) print(tabulate(df_reverse_sigma, headers=\u0026#39;keys\u0026#39;, tablefmt=\u0026#39;fancy_grid\u0026#39;)) return df_mu, df_sigma, df_reverse_mu, df_reverse_sigma def visualize_results(results): fig, axes = plt.subplots(2, 2, figsize=(16, 12)) axes[0,0].axhline(y=results[\u0026#39;true_grad_mu\u0026#39;], color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;-\u0026#39;, label=\u0026#39;True KL\u0026#39;) sns.violinplot(data=[results[\u0026#39;k1_grads_mu\u0026#39;], results[\u0026#39;k2_grads_mu\u0026#39;], results[\u0026#39;k3_grads_mu\u0026#39;]], ax=axes[0,0]) axes[0,0].set_title(\u0026#39;Gradient of KL Divergence with Respect to Mean Parameter\u0026#39;) axes[0,0].set_xticks([0,1,2]) axes[0,0].set_xticklabels([\u0026#39;k1\u0026#39;, \u0026#39;k2\u0026#39;, \u0026#39;k3\u0026#39;]) axes[0,0].set_ylabel(\u0026#39;Gradient Value\u0026#39;) axes[0,0].legend() axes[0,1].axhline(y=results[\u0026#39;true_grad_sigma\u0026#39;], color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;-\u0026#39;, label=\u0026#39;True KL\u0026#39;) sns.violinplot(data=[results[\u0026#39;k1_grads_sigma\u0026#39;], results[\u0026#39;k2_grads_sigma\u0026#39;], results[\u0026#39;k3_grads_sigma\u0026#39;]], ax=axes[0,1]) axes[0,1].set_title(\u0026#39;Gradient of KL Divergence with Respect to Standard Deviation Parameter\u0026#39;) axes[0,1].set_xticks([0,1,2]) axes[0,1].set_xticklabels([\u0026#39;k1\u0026#39;, \u0026#39;k2\u0026#39;, \u0026#39;k3\u0026#39;]) axes[0,1].set_ylabel(\u0026#39;Gradient Value\u0026#39;) axes[0,1].legend() axes[1,0].axhline(y=results[\u0026#39;true_reverse_grad_mu\u0026#39;], color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;-\u0026#39;, label=\u0026#39;True Reverse KL\u0026#39;) sns.violinplot(data=[results[\u0026#39;reverse_k1_grads_mu\u0026#39;], results[\u0026#39;reverse_k2_grads_mu\u0026#39;]], ax=axes[1,0]) axes[1,0].set_title(\u0026#39;Gradient of Reverse KL Divergence with Respect to Mean Parameter\u0026#39;) axes[1,0].set_xticks([0,1]) axes[1,0].set_xticklabels([\u0026#39;reverse_k1\u0026#39;, \u0026#39;reverse_k2\u0026#39;]) axes[1,0].set_ylabel(\u0026#39;Gradient Value\u0026#39;) axes[1,0].legend() axes[1,1].axhline(y=results[\u0026#39;true_reverse_grad_sigma\u0026#39;], color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;-\u0026#39;, label=\u0026#39;True Reverse KL\u0026#39;) sns.violinplot(data=[results[\u0026#39;reverse_k1_grads_sigma\u0026#39;], results[\u0026#39;reverse_k2_grads_sigma\u0026#39;]], ax=axes[1,1]) axes[1,1].set_title(\u0026#39;Gradient of Reverse KL Divergence with Respect to Standard Deviation Parameter\u0026#39;) axes[1,1].set_xticks([0,1]) axes[1,1].set_xticklabels([\u0026#39;reverse_k1\u0026#39;, \u0026#39;reverse_k2\u0026#39;]) axes[1,1].set_ylabel(\u0026#39;Gradient Value\u0026#39;) axes[1,1].legend() plt.tight_layout() plt.show() # Print mean comparison print(\u0026#34;Average gradient value comparison (mean parameter):\u0026#34;) print(f\u0026#34;True KL gradient: {results[\u0026#39;true_grad_mu\u0026#39;]:.6f}\u0026#34;) print(f\u0026#34;k1 average gradient: {np.mean(results[\u0026#39;k1_grads_mu\u0026#39;]):.6f}\u0026#34;) print(f\u0026#34;k2 average gradient: {np.mean(results[\u0026#39;k2_grads_mu\u0026#39;]):.6f}\u0026#34;) print(f\u0026#34;k3 average gradient: {np.mean(results[\u0026#39;k3_grads_mu\u0026#39;]):.6f}\u0026#34;) print(\u0026#34;\\nAverage gradient value comparison (standard deviation parameter):\u0026#34;) print(f\u0026#34;True KL gradient: {results[\u0026#39;true_grad_sigma\u0026#39;]:.6f}\u0026#34;) print(f\u0026#34;k1 average gradient: {np.mean(results[\u0026#39;k1_grads_sigma\u0026#39;]):.6f}\u0026#34;) print(f\u0026#34;k2 average gradient: {np.mean(results[\u0026#39;k2_grads_sigma\u0026#39;]):.6f}\u0026#34;) print(f\u0026#34;k3 average gradient: {np.mean(results[\u0026#39;k3_grads_sigma\u0026#39;]):.6f}\u0026#34;) print(\u0026#34;\\nReverse KL - Average gradient value comparison (mean parameter):\u0026#34;) print(f\u0026#34;True Reverse KL gradient: {results[\u0026#39;true_reverse_grad_mu\u0026#39;]:.6f}\u0026#34;) print(f\u0026#34;reverse_k1 average gradient: {np.mean(results[\u0026#39;reverse_k1_grads_mu\u0026#39;]):.6f}\u0026#34;) print(f\u0026#34;reverse_k2 average gradient: {np.mean(results[\u0026#39;reverse_k2_grads_mu\u0026#39;]):.6f}\u0026#34;) print(\u0026#34;\\nReverse KL - Average gradient value comparison (standard deviation parameter):\u0026#34;) print(f\u0026#34;True Reverse KL gradient: {results[\u0026#39;true_reverse_grad_sigma\u0026#39;]:.6f}\u0026#34;) print(f\u0026#34;reverse_k1 average gradient: {np.mean(results[\u0026#39;reverse_k1_grads_sigma\u0026#39;]):.6f}\u0026#34;) print(f\u0026#34;reverse_k2 average gradient: {np.mean(results[\u0026#39;reverse_k2_grads_sigma\u0026#39;]):.6f}\u0026#34;) # Add nice table output print() df = create_nice_table(results) if __name__ == \u0026#34;__main__\u0026#34;: sample_size = 50000 num_trials = 1000 results = estimate_gradients(sample_size, num_trials) visualize_results(results) Additional Notes The variance reduction approach for k2 loss can follow Schulman (2020)\u0026rsquo;s analysis by introducing a zero-mean statistic and solving with the method of undetermined coefficients.\nFirst, solve for $\\theta$ component-wise:\n$$ \\begin{align*} \\lambda_i^* \u0026amp;= \\argmin_\\lambda E_{X\\sim\\pi_\\theta}\\left[(\\nabla_{\\theta_i}\\log\\pi_{\\theta}(X))^2 \\cdot \\left(\\log\\left(\\frac{\\pi_\\theta(X)}{\\pi_{ref}(X)}\\right) - \\lambda\\right)^2\\right]\\\\ \u0026amp;= \\frac{E_{\\pi_\\theta}\\left[(\\nabla_{\\theta_i}\\log\\pi_{\\theta}(X))^2\\cdot \\left(\\log\\left(\\frac{\\pi_\\theta(X)}{\\pi_{ref}(X)}\\right)\\right)^2\\right]}{E_{\\pi_\\theta}\\left[(\\nabla_{\\theta_i}\\log\\pi_{\\theta}(X))^2\\right]}. \\end{align*} $$\nThen apply the modified estimator component-wise,\n$$ \\nabla_{\\theta_i} = \\nabla_{\\theta_i} \\frac{1}{2}(\\log\\pi_\\theta(X) - \\log\\pi_{ref}(X) - \\lambda_i^*)^2. $$\nHowever, this approach is quite complicated. First, $\\lambda$ is difficult to estimate as it requires computing the score (log prob gradient) first, potentially necessitating two backward passes. Second, each component needs to be estimated separately.\nAs an alternative, we could consider using the previous step\u0026rsquo;s score and a single $\\lambda$ estimator:\n$$ \\hat{\\lambda}^* = \\frac{\\sum_i|\\nabla_{\\theta_{old}}\\log\\pi_{\\theta}(X_i)|_2^2\\cdot \\left(\\log\\left(\\frac{\\pi_\\theta(X_i)}{\\pi_{ref}(X_i)}\\right)\\right)^2}{\\sum_i|\\nabla_{\\theta_{old}}\\log\\pi_{\\theta}(X_i)|_2^2}. $$\nHowever, this remains complex as it requires per-sample gradient norms and introduces some bias, which may not be worth the effort. The effectiveness of KL loss itself is uncertain, and whether additional variance reduction is needed here is debatable - perhaps k2 is already sufficient.\nCitation Please cite this work as:\nYang, Xiaobo. (Mar 2025). Gradient Estimation of KL Divergence in Large Language Model Reinforcement Learning. Xiabo\u0026rsquo;s Blog.\nhttps://xiaobo-yang.github.io/posts/kl_grad/.\nOr in BibTeX format:\n@article{yang2025klgradient, title = \u0026#34;Gradient Estimation of KL Divergence in Large Language Model Reinforcement Learning.\u0026#34;, author = \u0026#34;Yang, Xiaobo\u0026#34;, journal = \u0026#34;xiaobo-yang.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Mar\u0026#34;, url = \u0026#34;https://xiaobo-yang.github.io/posts/kl_grad/\u0026#34; } References [1] John Schulman \u0026ldquo;Approximating KL Divergence.\u0026rdquo; 2020.\n[2] Guo et al. \u0026ldquo;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\u0026rdquo; 2025.\n","permalink":"https://xiaobo-yang.github.io/posts/kl_grad/","summary":"\u003ch1 id=\"theory\"\u003eTheory\u003c/h1\u003e\n\u003cp\u003eIn large language model reinforcement learning training, there are two ways to incorporate KL divergence constraints: reward shaping and KL loss. The former directly adds KL estimation to the reward value as a new reward $r \\leftarrow r - \\beta\\cdot KL$; the latter puts the KL estimation term in the loss function, computing $\\nabla_\\theta \\operatorname{KL}(\\pi_\\theta||\\pi_{ref})$ for backpropagation.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://joschu.net/blog/kl-approx.html\"\u003eSchulman (2020)\u003c/a\u003e proposed three methods for estimating KL divergence, among which k3 loss is considered a good estimator with both unbiased and low variance properties. DeepSeek\u0026rsquo;s GRPO algorithm \u003ca href=\"https://arxiv.org/abs/2501.12948\"\u003e(Guo et al. 2025)\u003c/a\u003e adopted this approach:\u003c/p\u003e","title":"Gradient Estimation of KL Divergence in Large Language Model Reinforcement Learning"},{"content":"If you find any mistakes in my blog posts or want to get in touch, contact me at 1800010745@pku.edu.cn\n","permalink":"https://xiaobo-yang.github.io/contact/","summary":"\u003cp\u003eIf you find any mistakes in my blog posts or want to get in touch, contact me at \u003ca href=\"mailto:1800010745@pku.edu.cn\"\u003e1800010745@pku.edu.cn\u003c/a\u003e\u003c/p\u003e","title":""}]