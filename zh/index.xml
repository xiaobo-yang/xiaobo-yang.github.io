<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>潇博的博客</title>
    <link>https://xiaobo-yang.github.io/zh/</link>
    <description>Recent content on 潇博的博客</description>
    <generator>Hugo -- 0.139.5</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 14 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://xiaobo-yang.github.io/zh/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>大模型强化学习训练中KL散度的梯度估计</title>
      <link>https://xiaobo-yang.github.io/zh/posts/kl_grad_zh/</link>
      <pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://xiaobo-yang.github.io/zh/posts/kl_grad_zh/</guid>
      <description>&lt;h1 id=&#34;理论&#34;&gt;理论&lt;/h1&gt;
&lt;p&gt;在大语言模型的强化学习训练中，有reward shaping和KL loss两种引入KL散度约束的方式，前者为直接对reward值加上kl估计，作为一个新的reward $r \leftarrow r - \beta\cdot KL$；另外一种为将KL估计量放到loss中，计算 $\nabla_\theta \operatorname{KL}(\pi_\theta||\pi_{ref})$，一起进行反向传播.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://joschu.net/blog/kl-approx.html&#34;&gt;Schulman (2020)&lt;/a&gt; 给出了三种KL散度的估计方式，其中k3 loss被认为是兼备unbiased和low variance的好估计量。，DeepSeek的GRPO算法 &lt;a href=&#34;https://arxiv.org/abs/2501.12948&#34;&gt;(Guo et al. 2025)&lt;/a&gt; 就使用了这种方式:&lt;/p&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://xiaobo-yang.github.io/figs/kl_in_r1_paper.png&#34;
         alt=&#34;Fig. 1. GRPO 算法. (来源：Guo et al. 2025)&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Fig. 1. GRPO 算法. (来源：&lt;a href=&#34;https://arxiv.org/abs/2501.12948&#34;&gt;Guo et al. 2025&lt;/a&gt;)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;不过，如果使用KL Loss，我们实际上是通过抽样来对KL散度的梯度做估计，这和&lt;a href=&#34;http://joschu.net/blog/kl-approx.html&#34;&gt;Schulman (2020)&lt;/a&gt; 的分析有一些区别。在训练中，我们构建了估计量后，将其直接作为loss进行反向传播求导，期望这仍然是一个很好的逼近：&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\nabla_\theta \widehat{\operatorname{KL}}(X)
&amp;amp;\approx \nabla_\theta \operatorname{KL}(\pi_\theta||\pi_{\theta_{ref}})=\nabla_\theta \int \pi_\theta(x)\cdot\log\left(\frac{\pi_\theta(x)}{\pi_{ref}(x)}\right)dx\\
&amp;amp;= \int \nabla_\theta\pi_\theta(x)\cdot\log\left(\frac{\pi_\theta(x)}{\pi_{ref}(x)}\right) + \pi_\theta(x)\nabla_\theta\log\pi_\theta(x)dx\\
&amp;amp;=\int \pi_\theta(x)\cdot\nabla_\theta\log\pi_\theta(x)\log\left(\frac{\pi_\theta(x)}{\pi_{ref}(x)}\right) + \pi_\theta(x)\cdot \frac{1}{\pi_\theta(x)}\nabla_\theta\pi_\theta(x)dx\\
&amp;amp;=E_{x\sim\pi_\theta}\left[\nabla_\theta\log\pi_\theta(X)\cdot\log\left(\frac{\pi_\theta(X)}{\pi_{ref}(X)}\right)\right].
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;但KL的无偏估计的梯度，未必是KL的梯度的无偏估计，&lt;/p&gt;
&lt;p&gt;$$
E_{\pi_\theta}[\nabla_\theta \widehat{\operatorname{KL}}(X)]
\neq \nabla_\theta E_{\pi_\theta}[ \widehat{\operatorname{KL}}(X)]
= \nabla_\theta \operatorname{KL}(\pi_\theta||\pi_{\theta_{ref}}).
$$&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>https://xiaobo-yang.github.io/zh/contact-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xiaobo-yang.github.io/zh/contact-zh/</guid>
      <description>&lt;p&gt;如果你发现我的博客文章中有什么错误，或者想和我交流，我的邮箱是：1800010745@pku.edu.cn&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
